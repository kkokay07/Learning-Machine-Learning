{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMn36jzrV8MEtm/1p8gcBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkokay07/Learning-Machine-Learning/blob/main/Regression%20Model/Polynomial_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Regression: Step-by-Step Guide\n",
        "\n",
        "## Introduction\n",
        "Polynomial regression extends linear regression by modeling nonlinear relationships using polynomial terms of features. It's particularly useful when the relationship between variables follows a curved pattern.\n",
        "\n",
        "## Mathematical Foundation\n",
        "Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε\n",
        "\n",
        "## Step-by-Step Implementation with Population Growth Example"
      ],
      "metadata": {
        "id": "T5BCo0gWxBoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error"
      ],
      "metadata": {
        "id": "6qTHWjmAxDMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Data Preparation"
      ],
      "metadata": {
        "id": "srsEipubxP3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample population growth data\n",
        "np.random.seed(42)\n",
        "years = np.linspace(0, 10, 100)\n",
        "population = 1000 + 150*years + 12*years**2 + np.random.normal(0, 100, 100)\n",
        "\n",
        "X = years.reshape(-1, 1)\n",
        "y = population"
      ],
      "metadata": {
        "id": "xvI1FHh1xTu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 1:**\n",
        "- Data preparation ensures clean, properly formatted input\n",
        "- Synthetic data helps understand the underlying patterns\n",
        "- Reshaping data (-1, 1) is crucial for sklearn compatibility\n",
        "- Random seed ensures reproducibility"
      ],
      "metadata": {
        "id": "SjU_WZMcxWzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Visualize Raw Data"
      ],
      "metadata": {
        "id": "L2sr5TfexgVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', alpha=0.5, label='Actual Data')\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('Population')\n",
        "plt.title('Population Growth Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCX8ViMOxZjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 2:**\n",
        "- Visual inspection helps identify patterns\n",
        "- Helps determine appropriate polynomial degree\n",
        "- Reveals potential outliers or anomalies\n",
        "- Guides model selection decisions"
      ],
      "metadata": {
        "id": "RaHXUFQ3xhz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Split the Data\n"
      ],
      "metadata": {
        "id": "BesC-0tDxmqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train.shape[0])\n",
        "print(\"Testing set size:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "h-7ko9zrxo83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 3:**\n",
        "- Prevents overfitting by evaluating on unseen data\n",
        "- Provides unbiased model evaluation\n",
        "- Test size of 20% is a common baseline\n",
        "- Random state ensures reproducible splits"
      ],
      "metadata": {
        "id": "0G-LwSipxtTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create Polynomial Features"
      ],
      "metadata": {
        "id": "BJfXGRvWx1L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different degrees\n",
        "degrees = [1, 2, 3, 4]\n",
        "models = {}\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "\n",
        "    # Transform features\n",
        "    X_train_poly = poly_features.fit_transform(X_train)\n",
        "    X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "    # Print feature names\n",
        "    feature_names = poly_features.get_feature_names_out(['X'])\n",
        "    print(f\"\\nDegree {degree} features:\", feature_names)\n",
        "\n",
        "    # Store transformers and features\n",
        "    models[degree] = {\n",
        "        'transformer': poly_features,\n",
        "        'X_train': X_train_poly,\n",
        "        'X_test': X_test_poly\n",
        "    }"
      ],
      "metadata": {
        "id": "3kfsNsLFxuLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 4:**\n",
        "- Creates higher-order terms for non-linear patterns\n",
        "- Different degrees capture different complexity levels\n",
        "- Feature names help interpret the model\n",
        "- Separate transformation of train and test prevents data leakage"
      ],
      "metadata": {
        "id": "Ysplqytyx49S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Train Models and Compare"
      ],
      "metadata": {
        "id": "0rCFDmCxyDtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for degree in degrees:\n",
        "    # Get transformed features\n",
        "    X_train_poly = models[degree]['X_train']\n",
        "    X_test_poly = models[degree]['X_test']\n",
        "\n",
        "    # Create and train model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train_poly)\n",
        "    y_test_pred = model.predict(X_test_poly)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "    # Store results\n",
        "    models[degree]['model'] = model\n",
        "    models[degree]['metrics'] = {\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'test_rmse': test_rmse\n",
        "    }\n",
        "\n",
        "    print(f\"\\nDegree {degree} Results:\")\n",
        "    print(f\"Training R²: {train_r2:.4f}\")\n",
        "    print(f\"Testing R²: {test_r2:.4f}\")\n",
        "    print(f\"Testing RMSE: {test_rmse:.2f}\")"
      ],
      "metadata": {
        "id": "B_4QaR32x-cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 5:**\n",
        "- Trains models of different complexities\n",
        "- Compares performance metrics\n",
        "- Helps identify optimal polynomial degree\n",
        "- Reveals potential overfitting"
      ],
      "metadata": {
        "id": "z7HDRprmyHzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Visualize Model Comparisons"
      ],
      "metadata": {
        "id": "h6HJR3QMyIxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Create range for smooth curve\n",
        "X_smooth = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
        "\n",
        "for i, degree in enumerate(degrees, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "\n",
        "    # Get model and transformer\n",
        "    model = models[degree]['model']\n",
        "    transformer = models[degree]['transformer']\n",
        "\n",
        "    # Transform smooth X and predict\n",
        "    X_smooth_poly = transformer.transform(X_smooth)\n",
        "    y_smooth = model.predict(X_smooth_poly)\n",
        "\n",
        "    # Plot\n",
        "    plt.scatter(X, y, color='blue', alpha=0.5, label='Actual Data')\n",
        "    plt.plot(X_smooth, y_smooth, color='red', label=f'Degree {degree}')\n",
        "    plt.xlabel('Years')\n",
        "    plt.ylabel('Population')\n",
        "    plt.title(f'Polynomial Regression (Degree {degree})\\nTest R²: {models[degree][\"metrics\"][\"test_r2\"]:.4f}')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_zffJt5kyQTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 6:**\n",
        "- Visual comparison of different models\n",
        "- Shows potential overfitting in higher degrees\n",
        "- Helps in selecting optimal model\n",
        "- Validates numerical metrics visually"
      ],
      "metadata": {
        "id": "dvkRXT7UyUaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Residual Analysis"
      ],
      "metadata": {
        "id": "A-5b8IkDyZi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Choose best model (assume degree 2 for example)\n",
        "best_degree = 2\n",
        "best_model = models[best_degree]['model']\n",
        "X_test_poly = models[best_degree]['X_test']\n",
        "y_pred = best_model.predict(X_test_poly)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Residual plot\n",
        "plt.subplot(121)\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "# Residual distribution\n",
        "plt.subplot(122)\n",
        "plt.hist(residuals, bins=20, alpha=0.7)\n",
        "plt.xlabel('Residual Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Residual Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RSD38H9AyaG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 7:**\n",
        "- Validates model assumptions\n",
        "- Identifies potential patterns in errors\n",
        "- Checks for homoscedasticity\n",
        "- Ensures residual normality"
      ],
      "metadata": {
        "id": "KueIDX8KyfNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Feature Importance Analysis"
      ],
      "metadata": {
        "id": "ZzleOu9ayjNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_coefficients(degree):\n",
        "    model = models[degree]['model']\n",
        "    transformer = models[degree]['transformer']\n",
        "    feature_names = transformer.get_feature_names_out(['X'])\n",
        "\n",
        "    coefficients = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': model.coef_\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(coefficients['Feature'], coefficients['Coefficient'])\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Coefficient Value')\n",
        "    plt.title(f'Feature Coefficients (Degree {degree})')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_coefficients(2)  # Plot for degree 2"
      ],
      "metadata": {
        "id": "k3vum7i9yf-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 8:**\n",
        "- Shows contribution of each polynomial term\n",
        "- Helps interpret model behavior\n",
        "- Identifies dominant terms\n",
        "- Guides feature selection"
      ],
      "metadata": {
        "id": "HmPIsz4aypOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Model Validation"
      ],
      "metadata": {
        "id": "0-h73UEoytHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Choose best degree model\n",
        "best_degree = 2\n",
        "X_poly = models[best_degree]['transformer'].fit_transform(X)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(LinearRegression(), X_poly, y, cv=5)\n",
        "\n",
        "print(\"\\nCross-Validation Results:\")\n",
        "print(f\"Mean R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
      ],
      "metadata": {
        "id": "MOFm_JPlytsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 9:**\n",
        "- Validates model stability\n",
        "- Provides robust performance estimates\n",
        "- Helps detect overfitting\n",
        "- Ensures model generalization"
      ],
      "metadata": {
        "id": "HFGEAlMzyycX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Prediction on New Data"
      ],
      "metadata": {
        "id": "P9l0mGGEy3TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(years_new, degree=2):\n",
        "    # Reshape input\n",
        "    X_new = np.array(years_new).reshape(-1, 1)\n",
        "\n",
        "    # Transform features\n",
        "    transformer = models[degree]['transformer']\n",
        "    X_new_poly = transformer.transform(X_new)\n",
        "\n",
        "    # Make prediction\n",
        "    model = models[degree]['model']\n",
        "    predictions = model.predict(X_new_poly)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example prediction\n",
        "future_years = [11, 12, 13]\n",
        "predictions = make_prediction(future_years)\n",
        "print(\"\\nPredictions for future years:\", predictions)"
      ],
      "metadata": {
        "id": "4GgBdAbfy2jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Step 10:**\n",
        "- Demonstrates practical model usage\n",
        "- Provides prediction interface\n",
        "- Enables model deployment\n",
        "- Facilitates business decision-making"
      ],
      "metadata": {
        "id": "uw7LgqAEy9SX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Issues and Solutions\n",
        "\n",
        "1. **Overfitting**\n",
        "   - Solution: Use regularization\n",
        "   - Monitor train vs test performance\n",
        "   - Choose appropriate polynomial degree\n",
        "   - Use cross-validation\n",
        "\n",
        "2. **Feature Scaling**\n",
        "   - Important for higher degrees\n",
        "   - Prevents numerical instability\n",
        "   - Improves optimization\n",
        "   - Standardize or normalize features\n",
        "\n",
        "3. **Degree Selection**\n",
        "   - Use cross-validation\n",
        "   - Consider business context\n",
        "   - Balance complexity vs performance\n",
        "   - Monitor for overfitting\n",
        "\n",
        "4. **Extrapolation**\n",
        "   - Be cautious with predictions outside data range\n",
        "   - Consider confidence intervals\n",
        "   - Validate with domain knowledge\n",
        "   - Monitor prediction uncertainty"
      ],
      "metadata": {
        "id": "7rCo6vcXzCVz"
      }
    }
  ]
}